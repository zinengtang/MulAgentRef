# spatial

# SceneSpeak

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Python Version](https://img.shields.io/badge/python-3.7%2B-blue.svg)
![GitHub stars](https://img.shields.io/github/stars/yourusername/SceneSpeak.svg)
![GitHub forks](https://img.shields.io/github/forks/yourusername/SceneSpeak.svg)

## Table of Contents

- [ğŸ“– Introduction](#-introduction)
- [ğŸš€ Features](#-features)
- [ğŸ“ Dataset](#-dataset)
- [âš™ï¸ Installation](#ï¸-installation)
- [ğŸ› ï¸ Usage](#ï¸-usage)
- [ğŸ“Š Evaluation](#-evaluation)
- [ğŸ“„ License](#-license)
- [ğŸ“š Citation](#-citation)
- [ğŸ“¬ Contact](#-contact)

## ğŸ“– Introduction

**** is a comprehensive dataset and platform designed to facilitate research in multi-perspective referential communication within photorealistic 3D environments. This project enables the study of how embodied agents (both human and AI) communicate about their surroundings from different viewpoints, aiming to achieve *communicative success*â€”where the listener accurately understands the speaker's intended referent.

![SceneSpeak Overview](figs/teaser.png)

## ğŸš€ Features

- **Multi-Perspective Communication:** Simulates scenarios where speaker and listener have different viewpoints within the same 3D environment.
- **Photorealistic 3D Environments:** Utilizes neural radiance fields (NeRF) for realistic scene generation.
- **Comprehensive Dataset:** Large-scale collection of referential expressions and corresponding 3D scenes.
- **Model Benchmarking:** Evaluation of state-of-the-art vision-language models against human performance.
- **Interactive Tools:** Platform to adjust task difficulty and agent configurations for diverse research needs.
- **Open-Source:** Fully open for community contributions, enhancements, and integrations.

## ğŸ“ Dataset

### Overview

The SceneSpeak dataset comprises:

- **2,970** human-written referring expressions.
- **1,485** generated 3D scenes.
- **27,504** sampled scenes with varying agent perspectives and referent placements.

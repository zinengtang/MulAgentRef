# spatial

# SceneSpeak

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Python Version](https://img.shields.io/badge/python-3.7%2B-blue.svg)
![GitHub stars](https://img.shields.io/github/stars/yourusername/SceneSpeak.svg)
![GitHub forks](https://img.shields.io/github/forks/yourusername/SceneSpeak.svg)

## Table of Contents

- [📖 Introduction](#-introduction)
- [🚀 Features](#-features)
- [📁 Dataset](#-dataset)
- [⚙️ Installation](#️-installation)
- [🛠️ Usage](#️-usage)
- [📊 Evaluation](#-evaluation)
- [📄 License](#-license)
- [📚 Citation](#-citation)
- [📬 Contact](#-contact)

## 📖 Introduction

**** is a comprehensive dataset and platform designed to facilitate research in multi-perspective referential communication within photorealistic 3D environments. This project enables the study of how embodied agents (both human and AI) communicate about their surroundings from different viewpoints, aiming to achieve *communicative success*—where the listener accurately understands the speaker's intended referent.

![SceneSpeak Overview](figs/teaser.png)

## 🚀 Features

- **Multi-Perspective Communication:** Simulates scenarios where speaker and listener have different viewpoints within the same 3D environment.
- **Photorealistic 3D Environments:** Utilizes neural radiance fields (NeRF) for realistic scene generation.
- **Comprehensive Dataset:** Large-scale collection of referential expressions and corresponding 3D scenes.
- **Model Benchmarking:** Evaluation of state-of-the-art vision-language models against human performance.
- **Interactive Tools:** Platform to adjust task difficulty and agent configurations for diverse research needs.
- **Open-Source:** Fully open for community contributions, enhancements, and integrations.

## 📁 Dataset

### Overview

The SceneSpeak dataset comprises:

- **2,970** human-written referring expressions.
- **1,485** generated 3D scenes.
- **27,504** sampled scenes with varying agent perspectives and referent placements.

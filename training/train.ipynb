{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning with Llava as teacher/student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_prompt_task = \"\"\"\n",
    "Enhanced Prompt\n",
    "Identify the target sphere according to the description\n",
    "Outline its position using a bounding box and provide its coordinates in the format:\n",
    "\n",
    "x0 (left)\n",
    "y0 (top)\n",
    "x1 (right)\n",
    "y1 (bottom)\n",
    "\n",
    "Format for Response:\n",
    "\"Bounding box coordinates: [x0, y0, x1, y1]\"\n",
    "\"\"\"\n",
    "init_prompt_instruct = \"\"\"\n",
    "Describe the location of the blue sphere relative to the environment features.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/terran/miniconda3/envs/habitat/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.15it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.15it/s]\n",
      "Some kwargs in processor config are unused and will not have any effect: num_additional_image_tokens. \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "\n",
    "device = \"cuda:1\"\n",
    "# Initialize models and LoRA configuration\n",
    "model_name = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "def create_lora_model(base_model, device, trainable=True):\n",
    "    \"\"\"Create a LoRA-adapted model\"\"\"\n",
    "    # LoRA configuration\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,  # rank\n",
    "        lora_alpha=32,  # alpha scaling\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"k_proj\",\n",
    "            \"v_proj\",\n",
    "        ],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    # Add LoRA adapters\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    return model\n",
    "\n",
    "# Create base models\n",
    "base_speaker_model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ").to(device)\n",
    "\n",
    "listener_model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ").to(device).eval()\n",
    "\n",
    "# Apply LoRA\n",
    "speaker_model = create_lora_model(base_speaker_model, 'cuda:7', trainable=True)\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "# dataset = load you own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummy_image():\n",
    "    \"\"\"Creates a dummy image tensor. Replace with actual image data as needed.\"\"\"\n",
    "    # For example, a random RGB image of size 224x224\n",
    "    return torch.randn(3, 224, 224)\n",
    "\n",
    "# Example Dataset with 2 Dummy Examples\n",
    "dataset = [\n",
    "    {\n",
    "        'speaker_view_image': create_dummy_image(),\n",
    "        'listener_view_image': create_dummy_image(),\n",
    "        'listener_target_bbox': [50, 50, 150, 150],\n",
    "        'listener_distractor_0_bbox': [30, 30, 100, 100],\n",
    "        'listener_distractor_1_bbox': [160, 160, 220, 220],\n",
    "    },\n",
    "    {\n",
    "        'speaker_view_image': create_dummy_image(),\n",
    "        'listener_view_image': create_dummy_image(),\n",
    "        'listener_target_bbox': [60, 60, 140, 140],\n",
    "        'listener_distractor_0_bbox': [20, 20, 80, 80],\n",
    "        'listener_distractor_1_bbox': [170, 170, 230, 230],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "import copy\n",
    "\n",
    "class DualLLaVATrainer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        speaker_model, \n",
    "        listener_model, \n",
    "        processor, \n",
    "        learning_rate=1e-5, \n",
    "        gamma=0.99, \n",
    "        epsilon=0.2, \n",
    "        c1=1, \n",
    "        c2=0.01\n",
    "    ):\n",
    "        self.speaker_model = speaker_model\n",
    "        self.listener_model = listener_model\n",
    "        self.processor = processor\n",
    "        self.optimizer = torch.optim.Adam(speaker_model.parameters(), lr=learning_rate)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "        \n",
    "        # Freeze listener model\n",
    "        for param in self.listener_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.listener_model.eval()\n",
    "        \n",
    "    def calculate_iou(self, pred_box, gt_box):\n",
    "        \"\"\"Calculate IoU between predicted and ground truth boxes\"\"\"\n",
    "        if isinstance(gt_box, str):\n",
    "            gt_box = eval(gt_box)\n",
    "        \n",
    "        pred_x1, pred_y1, pred_x2, pred_y2 = pred_box\n",
    "        gt_x1, gt_y1, gt_x2, gt_y2 = gt_box\n",
    "        \n",
    "        x1 = max(pred_x1, gt_x1)\n",
    "        y1 = max(pred_y1, gt_y1)\n",
    "        x2 = min(pred_x2, gt_x2)\n",
    "        y2 = min(pred_y2, gt_y2)\n",
    "        \n",
    "        intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "        \n",
    "        pred_area = (pred_x2 - pred_x1) * (pred_y2 - pred_y1)\n",
    "        gt_area = (gt_x2 - gt_x1) * (gt_y2 - gt_y1)\n",
    "        union = pred_area + gt_area - intersection\n",
    "        \n",
    "        return intersection / (union + 1e-6)\n",
    "\n",
    "    def get_bbox_from_output(self, output_text):\n",
    "        \"\"\"Extract bounding box coordinates from model output\"\"\"\n",
    "        try:\n",
    "            import re\n",
    "            coords = re.findall(r'\\[([\\d\\.,\\s]+)\\]', output_text)\n",
    "            if coords:\n",
    "                return [float(x) for x in coords[-1].split(',')]\n",
    "            return None\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def get_listener_predictions(self, images, speaker_messages, batch):\n",
    "        \"\"\"Get bounding box predictions from listener model by selecting from available boxes\"\"\"\n",
    "        boxes = [\n",
    "            batch[0]['listener_target_bbox'],\n",
    "            batch[0]['listener_distractor_0_bbox'],\n",
    "            batch[0]['listener_distractor_1_bbox']\n",
    "        ]\n",
    "        # Format the boxes as choices\n",
    "        box_choices = f\"\"\"\n",
    "        Boxes:\n",
    "        A={boxes[0]} B={boxes[1]} C={boxes[2]}\n",
    "        Description: {speaker_messages[0]}\n",
    "        Choose box A, B, or C.\n",
    "        \"\"\"\n",
    "        # Get speaker outputs (old policy)\n",
    "        conversation = [\n",
    "            {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": box_choices},\n",
    "                {\"type\": \"image\"},\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "        prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "        # print(prompt)\n",
    "        inputs = processor(images=images[0], text=prompt, return_tensors='pt').to(self.listener_model.device).to(torch.bfloat16)\n",
    "        # Generate predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = self.listener_model.generate(**inputs, max_length=300)\n",
    "            texts = self.processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "            \n",
    "        # Parse selections and convert to boxes\n",
    "        selected_boxes = []\n",
    "        # print(texts[0])\n",
    "        for i, text in enumerate(texts):\n",
    "            # Extract the selection (A, B, or C)\n",
    "            if 'A' in text.upper() or 'CHOICE A' in text.upper() or 'SELECT A' in text.upper():\n",
    "                selected_boxes.append(eval(batch[i]['listener_target_bbox']))\n",
    "            elif 'B' in text.upper() or 'CHOICE B' in text.upper() or 'SELECT B' in text.upper():\n",
    "                selected_boxes.append(eval(batch[i]['listener_distractor_0_bbox']))\n",
    "            elif 'C' in text.upper() or 'CHOICE C' in text.upper() or 'SELECT C' in text.upper():\n",
    "                selected_boxes.append(eval(batch[i]['listener_distractor_1_bbox']))\n",
    "            else:\n",
    "                # Default to target box if no clear selection (you might want to handle this differently)\n",
    "                selected_boxes.append(eval(batch[i]['listener_distractor_0_bbox']))\n",
    "        \n",
    "        return selected_boxes\n",
    "\n",
    "    def compute_rewards(self, listener_boxes, gt_boxes):\n",
    "        \"\"\"Compute rewards based on listener's box predictions\"\"\"\n",
    "        rewards = []\n",
    "        for pred, gt in zip(listener_boxes, gt_boxes):\n",
    "            if pred is None:\n",
    "                rewards.append(-1.0)\n",
    "            else:\n",
    "                iou = self.calculate_iou(pred, gt)\n",
    "                rewards.append(iou)  # Use raw IOU as reward\n",
    "        return torch.tensor(rewards, device=self.speaker_model.device)\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        \"\"\"Single training step\"\"\"\n",
    "        speaker_images = [item['speaker_view_image'] for item in batch]\n",
    "        listener_images = [item['listener_view_image'] for item in batch]\n",
    "        gt_boxes = [item['listener_target_bbox'] for item in batch]\n",
    "        \n",
    "        # Get speaker outputs (old policy)\n",
    "        conversation = [\n",
    "            {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": init_prompt_instruct},\n",
    "                {\"type\": \"image\"},\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "        prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "        speaker_inputs = processor(images=speaker_images[0], text=prompt, return_tensors='pt').to(self.speaker_model.device).to(torch.bfloat16)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            old_outputs = self.speaker_model.generate(\n",
    "                **speaker_inputs,\n",
    "                max_length=300,\n",
    "                num_beams=1,\n",
    "                do_sample=True,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            old_logprobs = self.speaker_model(**speaker_inputs).logits\n",
    "            old_messages = self.processor.batch_decode(\n",
    "                old_outputs, \n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "            old_messages = [item.split('ASSISTANT: ')[-1][:100] for item in old_messages]\n",
    "        \n",
    "        # Get listener predictions and compute rewards\n",
    "        listener_boxes = self.get_listener_predictions(listener_images, old_messages, batch)\n",
    "        rewards = self.compute_rewards(listener_boxes, gt_boxes).to(torch.bfloat16)\n",
    "        # Store the old policy outputs\n",
    "        # print(rewards.shape)\n",
    "        old_policy = {\n",
    "            'logprobs': old_logprobs.detach(),\n",
    "            'rewards': rewards.detach(),\n",
    "        }\n",
    "        \n",
    "        # PPO update for speaker using stored values\n",
    "        for _ in range(3):\n",
    "            outputs = self.speaker_model(**speaker_inputs)\n",
    "            new_logprobs = outputs.logits\n",
    "            \n",
    "            # Compute policy ratio using stored logprobs\n",
    "            ratio = torch.exp(new_logprobs - old_policy['logprobs'])\n",
    "            clipped_ratio = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon)\n",
    "            # print(ratio.shape)\n",
    "            # Compute losses using stored rewards\n",
    "            policy_loss = -torch.min(\n",
    "                ratio * old_policy['rewards'], \n",
    "                clipped_ratio * old_policy['rewards']\n",
    "            ).mean()\n",
    "            value_loss = F.mse_loss(new_logprobs, old_policy['rewards'].unsqueeze(-1))\n",
    "            entropy_loss = -torch.mean(\n",
    "                torch.distributions.Categorical(logits=new_logprobs).entropy()\n",
    "            )\n",
    "            \n",
    "            total_loss = policy_loss + self.c1 * value_loss + self.c2 * entropy_loss\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        return total_loss.item(), rewards.mean().item()\n",
    "\n",
    "def train_dual_llava(num_epochs=10, batch_size=1):\n",
    "    trainer = DualLLaVATrainer(speaker_model, listener_model, processor)\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_losses = []\n",
    "        epoch_rewards = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(dataset), batch_size)):\n",
    "            try:\n",
    "                batch = [dataset[index] for index in range(i, i+batch_size)]\n",
    "                loss, reward = trainer.train_step(batch)\n",
    "                epoch_losses.append(loss)\n",
    "                epoch_rewards.append(reward)\n",
    "                \n",
    "                # Print examples periodically\n",
    "                if i % (batch_size * 10) == 0:\n",
    "                    print(\"\\nExample outputs:\")\n",
    "                    # Get speaker message\n",
    "                    conversation = [\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": [\n",
    "                                {\"type\": \"text\", \"text\": init_prompt_instruct},\n",
    "                                {\"type\": \"image\"},\n",
    "                            ],\n",
    "                        },\n",
    "                    ]\n",
    "                    prompt = processor.apply_chat_template(\n",
    "                        conversation, \n",
    "                        add_generation_prompt=True,\n",
    "                        max_length=200  # Limit input length\n",
    "                    )\n",
    "                    speaker_input = processor(\n",
    "                        images=batch[0]['speaker_view_image'], \n",
    "                        text=prompt, \n",
    "                        return_tensors='pt',\n",
    "                        max_length=200,  # Limit input length\n",
    "                        truncation=True\n",
    "                    ).to(trainer.speaker_model.device).to(torch.bfloat16)\n",
    "                    \n",
    "                    generated_message = trainer.processor.batch_decode(\n",
    "                        trainer.speaker_model.generate(\n",
    "                            **speaker_input, \n",
    "                            max_length=100,\n",
    "                            temperature=0.7\n",
    "                        ),\n",
    "                        skip_special_tokens=True\n",
    "                    )[0]\n",
    "                    generated_message = generated_message.split('ASSISTANT: ')[-1][:100]\n",
    "                    \n",
    "                    # Get listener prediction with bounding box choices\n",
    "                    boxes = [\n",
    "                        batch[0]['listener_target_bbox'],\n",
    "                        batch[0]['listener_distractor_0_bbox'],\n",
    "                        batch[0]['listener_distractor_1_bbox']\n",
    "                    ]\n",
    "                    box_choices = f\"\"\"Boxes:\n",
    "                        A={boxes[0]} B={boxes[1]} C={boxes[2]}\n",
    "                        Description: {generated_message}\n",
    "                        Choose box A, B, or C.\"\"\"\n",
    "                    \n",
    "                    conversation = [\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": [\n",
    "                                {\"type\": \"text\", \"text\": box_choices},\n",
    "                                {\"type\": \"image\"},\n",
    "                            ],\n",
    "                        },\n",
    "                    ]\n",
    "                    prompt = processor.apply_chat_template(\n",
    "                        conversation, \n",
    "                        add_generation_prompt=True,\n",
    "                        max_length=300  # Limit input length\n",
    "                    )\n",
    "                    listener_input = processor(\n",
    "                        images=batch[0]['listener_view_image'], \n",
    "                        text=prompt, \n",
    "                        return_tensors='pt',\n",
    "                        max_length=300,  # Limit input length\n",
    "                        truncation=True\n",
    "                    ).to(trainer.listener_model.device).to(torch.bfloat16)\n",
    "                    \n",
    "                    listener_output = trainer.processor.batch_decode(\n",
    "                        trainer.listener_model.generate(\n",
    "                            **listener_input, \n",
    "                            max_length=300\n",
    "                        ),\n",
    "                        skip_special_tokens=True\n",
    "                    )[0]\n",
    "                    \n",
    "                    print(f\"Speaker Message: {generated_message}\")\n",
    "                    print(f\"Listener Response: {listener_output}\")\n",
    "                    print(f\"Ground Truth Box: {batch[0]['listener_target_bbox']}\")\n",
    "                    if 'A' in listener_output.upper():\n",
    "                        print(\"Selected: Box A (Target)\")\n",
    "                    elif 'B' in listener_output.upper():\n",
    "                        print(\"Selected: Box B (Distractor 0)\")\n",
    "                    elif 'C' in listener_output.upper():\n",
    "                        print(\"Selected: Box C (Distractor 1)\")\n",
    "                    else:\n",
    "                        print(\"No clear selection\")\n",
    "                    print()\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Average Loss: {np.mean(epoch_losses):.4f}\")\n",
    "        print(f\"Average Reward: {np.mean(epoch_rewards):.4f}\")\n",
    "        \n",
    "        # Save speaker checkpoint\n",
    "        speaker_model.save_pretrained(f\"llava_speaker_dual_checkpoint_epoch\")\n",
    "train_dual_llava()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preference Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 48.16it/s]\n",
      "/home/terran/miniconda3/envs/habitat/lib/python3.9/site-packages/numpy-1.23.5-py3.9-linux-x86_64.egg/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/terran/miniconda3/envs/habitat/lib/python3.9/site-packages/numpy-1.23.5-py3.9-linux-x86_64.egg/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during training step: The image to be converted to a PIL image contains values outside the range [0, 1], got [-4.688628196716309, 4.558362007141113] which cannot be converted to uint8.\n",
      "Epoch 1/5\n",
      "Average Loss: nan\n",
      "Average Reward: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 332.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during training step: The image to be converted to a PIL image contains values outside the range [0, 1], got [-4.688628196716309, 4.558362007141113] which cannot be converted to uint8.\n",
      "Epoch 2/5\n",
      "Average Loss: nan\n",
      "Average Reward: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 321.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during training step: The image to be converted to a PIL image contains values outside the range [0, 1], got [-4.688628196716309, 4.558362007141113] which cannot be converted to uint8.\n",
      "Epoch 3/5\n",
      "Average Loss: nan\n",
      "Average Reward: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 327.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during training step: The image to be converted to a PIL image contains values outside the range [0, 1], got [-4.688628196716309, 4.558362007141113] which cannot be converted to uint8.\n",
      "Epoch 4/5\n",
      "Average Loss: nan\n",
      "Average Reward: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 311.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during training step: The image to be converted to a PIL image contains values outside the range [0, 1], got [-4.688628196716309, 4.558362007141113] which cannot be converted to uint8.\n",
      "Epoch 5/5\n",
      "Average Loss: nan\n",
      "Average Reward: nan\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, Dataset\n",
    "import copy\n",
    "import re\n",
    "\n",
    "class DualLLaVATrainer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        speaker_model, \n",
    "        listener_model, \n",
    "        processor, \n",
    "        learning_rate=1e-5, \n",
    "        gamma=0.99, \n",
    "        epsilon=0.2, \n",
    "        c1=1, \n",
    "        c2=0.01\n",
    "    ):\n",
    "        self.speaker_model = speaker_model\n",
    "        self.listener_model = listener_model\n",
    "        self.processor = processor\n",
    "        self.optimizer = torch.optim.Adam(speaker_model.parameters(), lr=learning_rate)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "        \n",
    "        # Freeze listener model\n",
    "        for param in self.listener_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.listener_model.eval()\n",
    "        \n",
    "    def calculate_iou(self, pred_box, gt_box):\n",
    "        \"\"\"Calculate IoU between predicted and ground truth boxes\"\"\"\n",
    "        if isinstance(gt_box, str):\n",
    "            gt_box = eval(gt_box)\n",
    "        \n",
    "        pred_x1, pred_y1, pred_x2, pred_y2 = pred_box\n",
    "        gt_x1, gt_y1, gt_x2, gt_y2 = gt_box\n",
    "        \n",
    "        x1 = max(pred_x1, gt_x1)\n",
    "        y1 = max(pred_y1, gt_y1)\n",
    "        x2 = min(pred_x2, gt_x2)\n",
    "        y2 = min(pred_y2, gt_y2)\n",
    "        \n",
    "        intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "        \n",
    "        pred_area = (pred_x2 - pred_x1) * (pred_y2 - pred_y1)\n",
    "        gt_area = (gt_x2 - gt_x1) * (gt_y2 - gt_y1)\n",
    "        union = pred_area + gt_area - intersection\n",
    "        \n",
    "        return intersection / (union + 1e-6)\n",
    "    \n",
    "    def get_bbox_from_output(self, output_text):\n",
    "        \"\"\"Extract bounding box coordinates from model output\"\"\"\n",
    "        try:\n",
    "            coords = re.findall(r'\\[([\\d\\.,\\s]+)\\]', output_text)\n",
    "            if coords:\n",
    "                return [float(x) for x in coords[-1].split(',')]\n",
    "            return None\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def get_listener_predictions(self, images, speaker_messages, batch):\n",
    "        \"\"\"Get bounding box predictions from listener model by selecting from available boxes\"\"\"\n",
    "        boxes = [\n",
    "            batch[0]['listener_target_bbox'],\n",
    "            batch[0]['listener_distractor_0_bbox'],\n",
    "            batch[0]['listener_distractor_1_bbox']\n",
    "        ]\n",
    "        # Format the boxes as choices\n",
    "        box_choices = f\"\"\"\n",
    "Boxes:\n",
    "A={boxes[0]} B={boxes[1]} C={boxes[2]}\n",
    "Description: {speaker_messages[0]}\n",
    "Choose box A, B, or C.\n",
    "\"\"\"\n",
    "        # Get speaker outputs (old policy)\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": box_choices},\n",
    "                    {\"type\": \"image\"},\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "        prompt = self.processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "        # Assuming images are pre-loaded; replace 'path/to/image' with actual image tensors if necessary\n",
    "        inputs = self.processor(images=images[0], text=prompt, return_tensors='pt').to(self.listener_model.device)\n",
    "        # Generate predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = self.listener_model.generate(**inputs, max_length=300)\n",
    "            texts = self.processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "            \n",
    "        # Parse selections and convert to boxes\n",
    "        selected_boxes = []\n",
    "        for i, text in enumerate(texts):\n",
    "            # Extract the selection (A, B, or C) using regex for robustness\n",
    "            match = re.search(r'\\b(A|B|C)\\b', text.upper())\n",
    "            if match:\n",
    "                choice = match.group(1)\n",
    "                if choice == 'A':\n",
    "                    selected_boxes.append(eval(batch[i]['listener_target_bbox']))\n",
    "                elif choice == 'B':\n",
    "                    selected_boxes.append(eval(batch[i]['listener_distractor_0_bbox']))\n",
    "                elif choice == 'C':\n",
    "                    selected_boxes.append(eval(batch[i]['listener_distractor_1_bbox']))\n",
    "            else:\n",
    "                # Default to distractor 0 if no clear selection\n",
    "                selected_boxes.append(eval(batch[i]['listener_distractor_0_bbox']))\n",
    "        \n",
    "        return selected_boxes\n",
    "    \n",
    "    def compute_rewards_ppl(self, batch, chosen_boxes):\n",
    "        \"\"\"Compute rewards based on Pairwise Preference Learning\"\"\"\n",
    "        rewards = []\n",
    "        for i, (chosen_box, example) in enumerate(zip(chosen_boxes, batch)):\n",
    "            target_box = eval(example['listener_target_bbox'])\n",
    "            intended_target = target_box\n",
    "            chosen_target = chosen_box\n",
    "            \n",
    "            if chosen_target == intended_target:\n",
    "                # Successful communication\n",
    "                rewards.append(1.0)\n",
    "            else:\n",
    "                # Communicative failure: reward is p(x|chosen) - p(x|intended)\n",
    "                # Compute p_speaker(refex | scene, referents, chosen)\n",
    "                # and p_speaker(refex | scene, referents, intended)\n",
    "                # Assuming we have a method to compute these probabilities\n",
    "                # For simplicity, using placeholder probabilities\n",
    "                # Replace with actual probability computations\n",
    "                # Example:\n",
    "                # p_chosen = compute_probability(example, chosen_target)\n",
    "                # p_intended = compute_probability(example, intended_target)\n",
    "                # reward = p_chosen - p_intended\n",
    "                # Here, we'll use dummy values\n",
    "                p_chosen = 0.6  # Placeholder\n",
    "                p_intended = 0.4  # Placeholder\n",
    "                reward = p_chosen - p_intended\n",
    "                rewards.append(reward)\n",
    "        \n",
    "        return torch.tensor(rewards, device=self.speaker_model.device)\n",
    "    \n",
    "    def compute_rewards(self, listener_boxes, batch):\n",
    "        \"\"\"Compute rewards based on listener's box predictions using PPL\"\"\"\n",
    "        rewards = []\n",
    "        for pred, example in zip(listener_boxes, batch):\n",
    "            if pred is None:\n",
    "                rewards.append(-1.0)\n",
    "            else:\n",
    "                target_box = eval(example['listener_target_bbox'])\n",
    "                chosen_box = pred\n",
    "                if chosen_box == target_box:\n",
    "                    rewards.append(1.0)\n",
    "                else:\n",
    "                    # Compute p_speaker(x|chosen) - p_speaker(x|intended)\n",
    "                    # Placeholder for actual computation\n",
    "                    # Replace with actual model probability computations\n",
    "                    # For demonstration, using a fixed difference\n",
    "                    p_chosen = 0.7  # Example probability for chosen target\n",
    "                    p_intended = 0.3  # Example probability for intended target\n",
    "                    reward = p_chosen - p_intended\n",
    "                    rewards.append(reward)\n",
    "        return torch.tensor(rewards, device=self.speaker_model.device)\n",
    "    \n",
    "    def train_step(self, batch):\n",
    "        \"\"\"Single training step with PPL\"\"\"\n",
    "        speaker_images = [item['speaker_view_image'] for item in batch]\n",
    "        listener_images = [item['listener_view_image'] for item in batch]\n",
    "        gt_boxes = [item['listener_target_bbox'] for item in batch]\n",
    "        \n",
    "        # Get speaker outputs (old policy)\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"Please describe the target object.\"},\n",
    "                    {\"type\": \"image\"},\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "        prompt = self.processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "        speaker_inputs = self.processor(images=speaker_images, text=prompt, return_tensors='pt', padding=True).to(self.speaker_model.device)\n",
    "        \n",
    "        # Generate referring expressions\n",
    "        with torch.no_grad():\n",
    "            old_outputs = self.speaker_model.generate(\n",
    "                **speaker_inputs,\n",
    "                max_length=100,\n",
    "                num_beams=1,\n",
    "                do_sample=True,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            old_messages = self.processor.batch_decode(\n",
    "                old_outputs, \n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "            old_messages = [msg.split('ASSISTANT: ')[-1].strip() for msg in old_messages]\n",
    "        \n",
    "        # Get listener predictions\n",
    "        listener_boxes = self.get_listener_predictions(listener_images, old_messages, batch)\n",
    "        \n",
    "        # Compute rewards using PPL\n",
    "        rewards = self.compute_rewards(listener_boxes, batch).detach()\n",
    "        \n",
    "        # Encode the inputs again for gradient computation\n",
    "        speaker_inputs = self.processor(images=speaker_images, text=prompt, return_tensors='pt', padding=True).to(self.speaker_model.device)\n",
    "        \n",
    "        # Forward pass to get logits\n",
    "        outputs = self.speaker_model(**speaker_inputs, labels=old_outputs)\n",
    "        log_probs = -F.cross_entropy(outputs.logits.view(-1, outputs.logits.size(-1)), old_outputs.view(-1), reduction='none')\n",
    "        log_probs = log_probs.view(old_outputs.size())  # Reshape to match outputs\n",
    "        \n",
    "        # Mask padding tokens\n",
    "        attention_mask = speaker_inputs['attention_mask']\n",
    "        log_probs = (log_probs * attention_mask).sum(dim=1)\n",
    "        \n",
    "        # Compute policy ratio\n",
    "        ratios = torch.exp(log_probs - log_probs.detach())\n",
    "        \n",
    "        # Compute surrogate losses\n",
    "        surrogate1 = ratios * rewards\n",
    "        surrogate2 = torch.clamp(ratios, 1 - self.epsilon, 1 + self.epsilon) * rewards\n",
    "        policy_loss = -torch.min(surrogate1, surrogate2).mean()\n",
    "        \n",
    "        # Value loss (optional, can be implemented if using a value network)\n",
    "        # Here, we'll omit it for simplicity\n",
    "        # value_loss = ...\n",
    "        \n",
    "        # Entropy loss for exploration\n",
    "        # Assuming logits are available\n",
    "        entropy = Categorical(logits=outputs.logits).entropy().mean()\n",
    "        entropy_loss = -entropy\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = policy_loss + self.c1 * 0 + self.c2 * entropy_loss  # Assuming no value loss\n",
    "        \n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return total_loss.item(), rewards.mean().item()\n",
    "\n",
    "def train_dual_llava(num_epochs=10, batch_size=1):\n",
    "    # Initialize models and processor\n",
    "    trainer = DualLLaVATrainer(speaker_model, listener_model, processor)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_losses = []\n",
    "        epoch_rewards = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(dataset), batch_size)):\n",
    "            try:\n",
    "                batch = [dataset[index] for index in range(i, min(i + batch_size, len(dataset)))]\n",
    "                loss, reward = trainer.train_step(batch)\n",
    "                epoch_losses.append(loss)\n",
    "                epoch_rewards.append(reward)\n",
    "                \n",
    "                # Print examples periodically\n",
    "                if i % (batch_size * 1) == 0:\n",
    "                    print(\"\\nExample outputs:\")\n",
    "                    # Get speaker message\n",
    "                    conversation = [\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": [\n",
    "                                {\"type\": \"text\", \"text\": \"Please describe the target object.\"},\n",
    "                                {\"type\": \"image\"},\n",
    "                            ],\n",
    "                        },\n",
    "                    ]\n",
    "                    prompt = processor.apply_chat_template(\n",
    "                        conversation, \n",
    "                        add_generation_prompt=True,\n",
    "                        max_length=200  # Limit input length\n",
    "                    )\n",
    "                    speaker_input = processor(\n",
    "                        images=batch[0]['speaker_view_image'], \n",
    "                        text=prompt, \n",
    "                        return_tensors='pt',\n",
    "                        max_length=200,  # Limit input length\n",
    "                        truncation=True,\n",
    "                        padding=True\n",
    "                    ).to(trainer.speaker_model.device)\n",
    "                    \n",
    "                    generated_outputs = trainer.speaker_model.generate(\n",
    "                        **speaker_input, \n",
    "                        max_length=100,\n",
    "                        temperature=0.7\n",
    "                    )\n",
    "                    generated_message = processor.batch_decode(\n",
    "                        generated_outputs,\n",
    "                        skip_special_tokens=True\n",
    "                    )[0]\n",
    "                    generated_message = generated_message.split('ASSISTANT: ')[-1].strip()[:100]\n",
    "                    \n",
    "                    # Get listener prediction with bounding box choices\n",
    "                    boxes = [\n",
    "                        batch[0]['listener_target_bbox'],\n",
    "                        batch[0]['listener_distractor_0_bbox'],\n",
    "                        batch[0]['listener_distractor_1_bbox']\n",
    "                    ]\n",
    "                    box_choices = f\"\"\"Boxes:\n",
    "A={boxes[0]} B={boxes[1]} C={boxes[2]}\n",
    "Description: {generated_message}\n",
    "Choose box A, B, or C.\"\"\"\n",
    "                    \n",
    "                    conversation = [\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": [\n",
    "                                {\"type\": \"text\", \"text\": box_choices},\n",
    "                                {\"type\": \"image\"},\n",
    "                            ],\n",
    "                        },\n",
    "                    ]\n",
    "                    prompt = processor.apply_chat_template(\n",
    "                        conversation, \n",
    "                        add_generation_prompt=True,\n",
    "                        max_length=300  # Limit input length\n",
    "                    )\n",
    "                    listener_input = processor(\n",
    "                        images=batch[0]['listener_view_image'], \n",
    "                        text=prompt, \n",
    "                        return_tensors='pt',\n",
    "                        max_length=300,  # Limit input length\n",
    "                        truncation=True,\n",
    "                        padding=True\n",
    "                    ).to(trainer.listener_model.device)\n",
    "                    \n",
    "                    listener_output = trainer.listener_model.generate(\n",
    "                        **listener_input, \n",
    "                        max_length=300\n",
    "                    )\n",
    "                    listener_response = processor.batch_decode(\n",
    "                        listener_output,\n",
    "                        skip_special_tokens=True\n",
    "                    )[0]\n",
    "                    \n",
    "                    print(f\"Speaker Message: {generated_message}\")\n",
    "                    print(f\"Listener Response: {listener_response}\")\n",
    "                    print(f\"Ground Truth Box: {batch[0]['listener_target_bbox']}\")\n",
    "                    if 'A' in listener_response.upper():\n",
    "                        print(\"Selected: Box A (Target)\")\n",
    "                    elif 'B' in listener_response.upper():\n",
    "                        print(\"Selected: Box B (Distractor 0)\")\n",
    "                    elif 'C' in listener_response.upper():\n",
    "                        print(\"Selected: Box C (Distractor 1)\")\n",
    "                    else:\n",
    "                        print(\"No clear selection\")\n",
    "                    print()\n",
    "            except Exception as e:\n",
    "                print(f\"Error during training step: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Average Loss: {np.mean(epoch_losses):.4f}\")\n",
    "        print(f\"Average Reward: {np.mean(epoch_rewards):.4f}\")\n",
    "        \n",
    "        # Save speaker checkpoint\n",
    "        speaker_model.save_pretrained(f\"llava_speaker_ppl_checkpoint_epoch_{epoch+1}\")\n",
    "\n",
    "# Example Usage\n",
    "# Ensure that the paths to the processor and models are correctly specified\n",
    "train_dual_llava(num_epochs=5, batch_size=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "model_name = \"llava-hf/llava-1.5-7b-hf\"\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16\n",
    ").to(device).to(torch.float16)\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "rephrase = False\n",
    "# dataset = load you own dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "from openai import OpenAI\n",
    "# client = OpenAI(api_key='your-key')\n",
    "\n",
    "def setup_lora_model():\n",
    "    # Initialize base model\n",
    "    model_name = \"llava-hf/llava-1.5-7b-hf\"\n",
    "    base_model = LlavaForConditionalGeneration.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16\n",
    "    ).to('cuda:7')\n",
    "    processor = AutoProcessor.from_pretrained(model_name)\n",
    "    \n",
    "    # Define LoRA configuration\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=8,  # rank of LoRA update matrices\n",
    "        lora_alpha=32,  # alpha scaling factor\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\n",
    "            \"q_proj\",\n",
    "            \"v_proj\",\n",
    "            \"k_proj\",\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    # Create PEFT model\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    return model, processor\n",
    "\n",
    "class LoRASpeakerTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        processor,\n",
    "        learning_rate=1e-4  # Higher learning rate for LoRA\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.processor = processor\n",
    "        # Only optimize LoRA parameters\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            [p for n, p in model.named_parameters() if \"lora\" in n.lower()],\n",
    "            lr=learning_rate\n",
    "        )\n",
    "        \n",
    "    def train_step(self, batch):\n",
    "        \"\"\"Single training step with LoRA parameters\"\"\"\n",
    "        # Get speaker image and reference message\n",
    "        speaker_image = batch[0]['speaker_view_image']\n",
    "\n",
    "        reference_message = batch[0]['human_speaker_message']\n",
    "        if rephrase:\n",
    "            chat_completion = client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"rephrase the below message: {reference_message}\",\n",
    "                    }\n",
    "                ],\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "            )\n",
    "            reference_message = chat_completion.choices[0].message.content\n",
    "\n",
    "        # Prepare conversation prompt\n",
    "        init_prompt_instruct = \"Describe the location of the blue sphere relative to the environment features.\"\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": init_prompt_instruct},\n",
    "                    {\"type\": \"image\"},\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "        prompt = self.processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "        \n",
    "        # Process inputs\n",
    "        inputs = self.processor(\n",
    "            images=speaker_image,\n",
    "            text=prompt,\n",
    "            return_tensors='pt',\n",
    "            max_length=200,\n",
    "            truncation=True\n",
    "        ).to(self.model.device)\n",
    "        \n",
    "        # Create labels\n",
    "        labels = inputs[\"input_ids\"].clone()\n",
    "        labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
    "        \n",
    "        # Convert inputs to bfloat16 while preserving input_ids and attention_mask\n",
    "        inputs = {\n",
    "            k: v.to(torch.float16) if k not in ['input_ids', 'attention_mask'] else v\n",
    "            for k, v in inputs.items()\n",
    "        }\n",
    "        inputs[\"labels\"] = labels\n",
    "        \n",
    "        with torch.autocast(\"cuda:7\", dtype=torch.float16):\n",
    "            outputs = self.model(**inputs)\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Generate prediction for monitoring\n",
    "        with torch.no_grad():\n",
    "            generated = self.model.generate(\n",
    "                **inputs,\n",
    "                max_length=100,\n",
    "                num_beams=1,\n",
    "                do_sample=True,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            generated_message = self.processor.batch_decode(\n",
    "                generated,\n",
    "                skip_special_tokens=True\n",
    "            )[0].split('ASSISTANT: ')[-1][:100]\n",
    "        \n",
    "        return loss.item(), reference_message, generated_message\n",
    "\n",
    "def train_speaker_lora(num_epochs=10):\n",
    "    # Initialize LoRA model and trainer\n",
    "    model, processor = setup_lora_model()\n",
    "    trainer = LoRASpeakerTrainer(model, processor)\n",
    "    \n",
    "    # Load dataset\n",
    "    dataset = load_dataset(\"ZinengTang/PersReFex\", split=\"validation\")\n",
    "    dataset = dataset.select(range(100))\n",
    "    \n",
    "    # Training loop\n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_losses = []\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for i in tqdm(range(len(dataset))):\n",
    "            try:\n",
    "                # Get single example\n",
    "                batch = [dataset[i]]\n",
    "                \n",
    "                # Training step\n",
    "                loss, reference, generated = trainer.train_step(batch)\n",
    "                epoch_losses.append(loss)\n",
    "                \n",
    "                # Print examples periodically\n",
    "                if i % 10 == 0:\n",
    "                    print(f\"\\nStep {i}\")\n",
    "                    print(f\"Loss: {loss:.4f}\")\n",
    "                    print(f\"Reference: {reference}\")\n",
    "                    print(f\"Generated: {generated}\")\n",
    "                    print(\"-\" * 50)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {i}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Epoch summary\n",
    "        avg_loss = np.mean(epoch_losses)\n",
    "        print(f\"\\nEpoch {epoch+1} Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            model.save_pretrained(f\"llava_speaker_lora_best_model\")\n",
    "        \n",
    "        # Regular checkpoint\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            model.save_pretrained(f\"llava_speaker_lora_checkpoint_epoch_{epoch+1}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_speaker_lora()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing/ Be sure the replace the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "import torch\n",
    "# Load base model\n",
    "base_model = LlavaForConditionalGeneration.from_pretrained(\n",
    "    \"llava-hf/llava-1.5-7b-hf\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
    "\n",
    "# Load LoRA adapter\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"ZinengTang/llava-lora-spatial\"\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "init_prompt_instruct = \"Describe the location of the blue sphere relative to the environment features.\"\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": init_prompt_instruct},\n",
    "            {\"type\": \"image\"},  # This will be replaced with the actual image\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "speaker_image = Image.open('/home/terran/projects/spatial/vlsim/source/embodied/final_data/output_data_1/images/0/speaker.jpg')\n",
    "prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "# print(prompt)\n",
    "# Process the input image and prompt\n",
    "inputs = processor(\n",
    "    images=speaker_image,\n",
    "    text=prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=256,\n",
    ").to('cuda:7')\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        pixel_values=inputs[\"pixel_values\"],\n",
    "        max_length=512,\n",
    "        num_beams=1,\n",
    "        do_sample=True,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    generated_message = processor.batch_decode(\n",
    "        generated, \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    print(generated_message)\n",
    "    generated_message = generated_message[0].split('ASSISTANT: ')[-1][:100]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "habitat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
